\section*{Abstract}

The last twenty years have brought many technological advancements. The massive worldwide adoption of the Internet has created a gigantic platform where uncountable amounts of information are produced and shared. Likewise, the shift of many industries into the digital plane enabled the generation and storage of huge volumes of data. Processing such volumes of data in a sensible and timely manner required innovative computational paradigms; this led to the creation of several successful distributed models that proved to be better suited for tasks of such magnitude. One example is Apache Spark, a fault-tolerant distributed processing framework that uses a shared memory abstraction in order to offer better performance.

However, as any other software, applications written for these big data frameworks are susceptible to bugs and errors. The difference in this case is that bugs in distributed software have other implications in comparison to the traditional single-node computing. For instance, mishandled errors can permeate to multiple nodes, potentially collapsing a whole cluster of computers. Furthermore, if the faulty distributed computations are executed in third-party cloud servers, monetary losses could be inflicted due to unnecessary use of the resources and service downtime.

Although several program testing techniques have been ported to the context of distributed programming and have been the subject of research studies, program analysis approaches have received less attention both in the industry and the academia. Formal methods and code analyses could also prove useful towards the goal of improving code quality and their automated nature could provide a mechanism for a continuous evaluation.

This work aims to clarify the applicability of model checking techniques, in particular symbolic execution, in the context of big data systems. For this purpose, it introduces \textit{JPF-SymSpark}, a symbolic execution framework for Apache Spark programs built as an extension of \acrlong{acr:jpf}. The main goal of \textit{JPF-SymSpark} is to generate reduced input datasets that offer full path coverage of the analyzed program. Such datasets can have several uses in the development process of a Spark application, for example, as input data for unit tests. The tool is able to symbolically execute Spark programs that handle primitive data types and Strings as their input datasets. It is also capable of chaining multiple Spark operations during a symbolic execution; providing the mechanisms for a complete analysis of a program.

The evaluation of the approach is twofold. First, a qualitative evaluation that aims to determine how compliant \textit{JPF-SymSpark} is in terms of a series of functional and non-functional requirements defined for a tool of such purpose. Second, a performance assessment of the iterative symbolic operations carried out during an analysis. Additionally, a discussion over the limitations of the approach is presented in order to establish the scope and capabilities of its current state. Furthermore, the study concludes with a general discussion on the applicability of model checking techniques in the context of Java big data systems based on the currently available tools and frameworks.