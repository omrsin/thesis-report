\TUsection{Apache Spark}
\label{sec:spark}

%TODO How to include somewhere that Spark was built in Scala \cite{WebScala2017}

Spark is a data processing framework that was first introduced in 2012~\cite{Zaharia2012a}. Similar to other systems, such as MapReduce~\cite{Dean2004} and Dryad~\cite{Isard2007}, it aims to offer a clean and flexible abstraction to distributed computations on large datasets. However, Spark offers two advantages in comparison to such systems: It makes use of a shared memory abstraction that improves performance by avoiding persisting intermediate sets. It also provides an efficient fault-tolerance mechanism, based on tracking coarse-grained operations, that can recover lost tasks with minimal impact.

The working units in Spark are called \textit{\acrlongpl{abb:rdd}}, better known as \acrshortpl{abb:rdd}. These units represent an immutable partitioned collection of elements in a distributed memory space. \acrshortpl{abb:rdd} can only be created through a set of deterministic operations, known as \textit{transformations} (e.g., \textit{map}, \textit{filter} and \textit{join}), that can be applied to both, raw data or other \acrshortpl{abb:rdd}. Transformations are not evaluated immediately, instead Spark keeps track of all the transformations applied to each \acrshort{abb:rdd} in a program so it can optimize their subsequent processing. Additionally, \acrshortpl{abb:rdd} can be operated to return a value or can be made persistent into storage. These kind of operations are known as \textit{actions} (e.g., \textit{count}, \textit{reduce} and \textit{save}), and they are the ones that trigger the processing of \acrshortpl{abb:rdd}.

To interact with the \acrshort{abb:rdd} abstraction, Spark provides several \acrshortpl{abb:api} for different programming languages such as Java, Scala, Python and recently R~\cite{Venkataraman2016}.

\begin{lstlisting}[
	language=Scala,
	float,
	caption={[Log processing with Spark] Entries in a log file are filtered, grouped and counted based on a common property.},
	label=lst:spark
]
val log = spark.textFile("*file*") /*# \label{lst:spark:ln:conf} #*/
val errors = log.filter(_.contains("ERROR")) 
	.map(error => (error.split('\t')(0),1))
	.reduceByKey(_+_)
errors.save()
\end{lstlisting}

Moreover, repeating the same transformations on the same dataset always yield the same RDD as a result; this property is necessary for the fault-tolerant mechanism. I just want to cite here \ref{lst:spark:ln:conf}

(Lineage or provenance) Capturing lineage or provenance information for data has long been a research topic in scientific computing and databases, for applications such as explaining results, allowing them to be reproduced by others, and recomputing data if a bug is found in a workflow or if a dataset is lost \cite{Zaharia2012a}

(Spark) Spark provides a convenient language-integrated programming interface in the Scala programming language. Spark exposes RDDs through a language-integrated API where each dataset is represented as an object and transformations are invoked using methods on these objects. \cite{Zaharia2012a}

(Spark) Iterative algorithms and interactive querying of data sets.

(Version)Be sure to mention the current version of Spark at the moment or the version used for the analysis.

(How operation starts) Programmers start by defining one or more RDDs through transformations on data in stable storage (e.g., map and filter) \cite{Zaharia2012a}

(Spark Interface in Scala) Spark provides the RDD abstraction through a language-integrated API in Scala, a statically typed functional programming language for the Java VM. \cite{Zaharia2012a}

(First order functions) Users provide arguments to RDD operations like map by passing closures (function literals). Scala represents each closure as a Java object, and these objects can be serialized and loaded on another node to pass the closure across the network. \cite{Zaharia2012a}

(Transformations and Actions supported in paper) Table 2: Transformations and actions available on RDDs in Spark. Seq[T] denotes a sequence of elements of type T \cite{Zaharia2012a}

(Graph representation) We propose a simple graph-based representation for RDDs that facilitates these goals. (The goal of connecting consecutive RDD operations) \cite{Zaharia2012a}

(Concept of narrow dependency. Maybe not useful here but I don't want to forget) First, narrow dependencies allow for pipelined execution on one cluster node, which can compute all the parent partitions. For example, one can apply a map followed by a filter on an element-by-element basis \cite{Zaharia2012a}

(Runs under Mesos maybe not necessary) The system runs over the Mesos cluster manager, allowing it to share resources with Hadoop, MPI and other applications \cite{Zaharia2012a}

(Mention that Spark has several libraries, machine learning, graph, stream, structured)

(Mention that Spark is now an Apache product but also under Databricks) Cite \cite{WebSpark2017}

Check R. Bose and J. Frew. Lineage retrieval for scientific data processing: a survey and Cheney, L. Chiticariu, andW.-C. Tan. Provenance in databases: Why, how, and where