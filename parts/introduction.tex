\TUchapter{Introduction}
\label{ch:introduction}

\TUsection{Problem Statement}

%\TUsection{Illustrative Example}

%\TUsection{Hypothesis}

%TODO: rephrase
This study aims to identify if \textit{symbolic execution techniques can be used in the context of big data frameworks to generate reduced input data sets that enforce full path coverage}.

%\TUsection{Research Questions}

The following research questions are relevant for this study:

\begin{enumerate}
	\item Is symbolic execution a suitable technique for analyzing programs in the context of Spark applications.
	\item What are the particular characteristics associated with the symbolic execution of a Spark program.
	\item Is there a symbolic execution framework that can be adapted to perform symbolic executions of Spark programs.
	\item If it exists, what are the advantages and disadvantages of such a framework in the context of Spark applications.	
\end{enumerate}

\TUsection{Contributions}
\label{sec:contributions}

This work introduces \textit{JPF-SymSpark}, a symbolic execution framework for Apache Spark programs built as an extension of \acrfull{acr:jpf}. The main goal of \textit{JPF-SymSpark} is to generate reduced input datasets that offer full path coverage of the analyzed program. Such datasets can have several uses in the development process of a Spark application, for example, the generation of input data for unit tests.

The tool is able to symbolically execute Spark programs that handle primitive data types and Strings as their input datasets. It is also capable of chaining multiple Spark operations during a symbolic execution; providing the mechanisms for a complete analysis of a program instead of a method-by-method approach. This reasoning over the inter-relation of Spark operations and the data flow among them is the most useful contribution of this work. To our knowledge, there has not been any study over the application of symbolic execution in big data frameworks.

\textit{JPF-SymSpark} is built on top of \acrfull{acr:spf}, a symbolic execution extension of \jpf{} for general Java programs. During the development of the tool, \spf{} presented unexpected behaviors when analyzing some programs. Most of these abnormal results were caused by common programming practices in Spark applications, for example, the use of anonymous classes and lambda expressions to represent the parameter functions passed to many of Spark's operations. The \spf{} extension was modified accordingly in order to cope with these scenarios. The modifications \spf{} are:

\begin{itemize}
	\item Detection of synthetic bridge methods
	\item Consistent ordering of String path conditions
	\item Improvements to the visitor pattern in the symbolic constraints
\end{itemize}

Some of these modifications were included in a patch ans submitted to the \spf{} administrator for revision. However, to the date this document was published they have not been included in the official source code.

A detailed explanation about the modifications can be found in appendix~A.